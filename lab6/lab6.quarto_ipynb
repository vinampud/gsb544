{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LAB 6\n",
        "Priya Inampudi"
      ],
      "id": "bd77d8ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install plotnine"
      ],
      "id": "3e52c5ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%echo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV"
      ],
      "id": "133183fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "players = pd.read_csv(\"https://www.dropbox.com/s/boshaqfgdjiaxh4/Hitters.csv?dl=1\")\n",
        "players.head()"
      ],
      "id": "6c14421e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "players.isnull().sum()\n",
        "players = players.dropna(subset=[\"Salary\"])\n",
        "players.isnull().sum() # since prediciting salary, okay to drop"
      ],
      "id": "edba75b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = players.drop(\"Salary\", axis = 1)\n",
        "y = players[\"Salary\"]"
      ],
      "id": "5502d596",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
        "         make_column_selector(dtype_include=object)),\n",
        "        (\"standardize\", StandardScaler(),\n",
        "         make_column_selector(dtype_include=np.number)),\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")"
      ],
      "id": "c2081041",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# functions\n",
        "\n",
        "def get_feature_names(fitted_preprocessor, X):\n",
        "    cat_sel = fitted_preprocessor.transformers_[0][2]\n",
        "    num_sel = fitted_preprocessor.transformers_[1][2]\n",
        "\n",
        "    cat_names = fitted_preprocessor.transformers_[0][1].get_feature_names_out(\n",
        "        input_features=X.loc[:, cat_sel].columns\n",
        "    )\n",
        "    num_names = X.loc[:, num_sel].columns.astype(str)\n",
        "    return np.r_[cat_names, num_names]"
      ],
      "id": "cfd11df6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Part 1: Different Model Specs**"
      ],
      "id": "85790798"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PART A\n",
        "\n",
        "ols_pipe = Pipeline([\n",
        "    (\"pre_processing\", ct),\n",
        "    (\"linear_regression\", LinearRegression())\n",
        "])\n",
        "\n",
        "ols_rmse = -(cross_val_score(ols_pipe, X, y, cv = 5, scoring = \"neg_root_mean_squared_error\").mean())\n",
        "ols_mse = ols_rmse**2\n",
        "print(\"\\nRMSE:\", ols_rmse)\n",
        "print(\"MSE:\", ols_mse, \"\\n\")"
      ],
      "id": "72de95fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ols_pipe.fit(X, y)\n",
        "\n",
        "feat_names = get_feature_names(ols_pipe.named_steps[\"pre_processing\"], X)\n",
        "ols_coefs = pd.Series(ols_pipe.named_steps[\"linear_regression\"].coef_[:len(feat_names)], index=feat_names)\n",
        "\n",
        "ols_top = ols_coefs.abs().sort_values(ascending=False).head(5)\n",
        "print(ols_coefs.loc[ols_top.index].sort_values(key=np.abs, ascending=False))"
      ],
      "id": "da95c603",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recent and career offensive productivity (Hits, CRuns, CRBI) drive salary up, while sheer opportunities without production (AtBat) weigh slightly negative.\n",
        "OLS captures real patterns but can over-emphasize correlated stats, leading to instability.\n"
      ],
      "id": "199b970a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PART B\n",
        "ridge_pipe = Pipeline([\n",
        "    (\"pre_processing\", ct),\n",
        "    (\"ridge\", Ridge(random_state=0))\n",
        "])\n",
        "\n",
        "param_grid = {\"ridge__alpha\": np.logspace(-3, 3, 13)} \n",
        "\n",
        "ridge = GridSearchCV(ridge_pipe, param_grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "ridge_fitted = ridge.fit(X, y)\n",
        "\n",
        "print(\"\\nBest alpha:\", ridge_fitted.best_params_[\"ridge__alpha\"])\n",
        "print(\"RMSE:\", -ridge_fitted.best_score_)\n",
        "print(\"MSE: \", (-ridge_fitted.best_score_) ** 2, \"\\n\")"
      ],
      "id": "a4d46e11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ridge_best = ridge_fitted.best_estimator_\n",
        "\n",
        "ridge_best.fit(X, y)\n",
        "\n",
        "ridge_feat_names = get_feature_names(ridge_best.named_steps[\"pre_processing\"], X)\n",
        "ridge_coefs = pd.Series(ridge_best.named_steps[\"ridge\"].coef_[:len(ridge_feat_names)],\n",
        "                       index=ridge_feat_names)\n",
        "\n",
        "ridge_top = ridge_coefs.abs().sort_values(ascending=False).head(5)\n",
        "print(ridge_coefs.loc[ridge_top.index].sort_values(key=np.abs, ascending=False))"
      ],
      "id": "25182c00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ridge retains the same direction of effects but reduces their magnitudes, making the model less sensitive to multicollinearity among batting variables.\n",
        "Its lower RMSE (339 vs 342) shows a modest improvement in predictive stability.\n"
      ],
      "id": "53650bb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PART C\n",
        "\n",
        "lasso_pipe = Pipeline([\n",
        "    (\"pre_processing\", ct),\n",
        "    (\"lasso\", Lasso(max_iter=10000, random_state=0))\n",
        "])\n",
        "\n",
        "param_grid = {\"lasso__alpha\": np.logspace(-3, 1, 9)} \n",
        "lasso = GridSearchCV(lasso_pipe, param_grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "lasso_fitted = lasso.fit(X, y)\n",
        "\n",
        "print(\"\\nBest alpha:\", lasso_fitted.best_params_[\"lasso__alpha\"])\n",
        "print(\"RMSE:\", -lasso_fitted.best_score_)\n",
        "print(\"MSE: \", (-lasso_fitted.best_score_)**2, \"\\n\")\n",
        "\n",
        "lasso_best = lasso_fitted.best_estimator_\n",
        "lasso_best.fit(X, y)\n",
        "\n",
        "lasso_feat_names = get_feature_names(lasso_best.named_steps[\"pre_processing\"], X)\n",
        "\n",
        "lasso_coef = pd.Series(lasso_best.named_steps[\"lasso\"].coef_[:len(lasso_feat_names)],\n",
        "                       index=lasso_feat_names)\n",
        "print(lasso_coef[lasso_coef != 0].sort_values(key=np.abs, ascending=False).head(5))"
      ],
      "id": "69609821",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LASSO shrinks small, redundant coefficients to 0, but not shown here.\n",
        "highlights a compact set of  approx 5 main predictors, the same core offensive stats as Ridge/OLS, giving nearly identical RMSE but with simpler interpretation."
      ],
      "id": "02c940d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PART D\n",
        "\n",
        "enet_pipe = Pipeline([\n",
        "    (\"pre_processing\", ct),\n",
        "    (\"elastic_net\", ElasticNet(max_iter=10000, random_state=0))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"elastic_net__alpha\":    np.logspace(-3, 1, 9),\n",
        "    \"elastic_net__l1_ratio\": np.linspace(0.1, 0.9, 9)\n",
        "}\n",
        "\n",
        "enet = GridSearchCV(enet_pipe, param_grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "enet_fitted = enet.fit(X, y)\n",
        "\n",
        "print(\"\\nBest params:\", enet_fitted.best_params_)\n",
        "print(\"RMSE:\", -enet_fitted.best_score_)\n",
        "print(\"MSE:\", (-enet_fitted.best_score_)**2, \"\\n\")\n",
        "\n",
        "enet_best = enet_fitted.best_estimator_\n",
        "enet_best.fit(X, y)\n",
        "\n",
        "enet_feat_names = get_feature_names(enet_best.named_steps[\"pre_processing\"], X)\n",
        "enet_coefs = pd.Series(enet_best.named_steps[\"elastic_net\"].coef_[:len(enet_feat_names)],\n",
        "                      index=enet_feat_names)\n",
        "\n",
        "enet_top = enet_coefs.abs().sort_values(ascending=False).head(5)\n",
        "print(enet_coefs.loc[enet_top.index].sort_values(key=np.abs, ascending=False))"
      ],
      "id": "ec4a96ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same main predictors remain, but coefficients are even smaller, more regularized.\n",
        "It achieved the best RMSE (approx. 339.0), but not exactly the biggest improvement over Ridge/LASSO but most balanced.\n",
        "\n",
        "\n",
        "**PART 2: Variable Selection**"
      ],
      "id": "8271bbc9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# based on results above\n",
        "best_num_1 = [\"Hits\"]\n",
        "best_num_5 = [\"Hits\", \"AtBat\", \"CRuns\", \"CRBI\", \"CWalks\"]\n",
        "best_cat_1 = [\"Division\"]"
      ],
      "id": "9b5f2056",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#functions to return each model\n",
        "def evaluate_model(pipe, X, y):\n",
        "    scores = cross_val_score(pipe, X, y, cv = 5, scoring = \"neg_root_mean_squared_error\")\n",
        "    rmse = -scores.mean()\n",
        "    mse = rmse**2\n",
        "    return rmse, mse\n",
        "\n",
        "def ridge_tuned(pre):\n",
        "    pipe = Pipeline([\n",
        "        (\"pre_processing\", pre),\n",
        "        (\"ridge\", Ridge(random_state=0))\n",
        "    ])\n",
        "    grid = {\"ridge__alpha\": np.logspace(-3, 3, 13)}\n",
        "    search = GridSearchCV(pipe, grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "    search.fit(X, y)\n",
        "    best = search.best_estimator_\n",
        "    rmse = -search.best_score_\n",
        "    mse = rmse**2\n",
        "    return best, rmse, mse\n",
        "\n",
        "def lasso_tuned(pre):\n",
        "    pipe = Pipeline([\n",
        "        (\"pre_processing\", pre),\n",
        "        (\"lasso\", Lasso(max_iter=10000, random_state=0))\n",
        "    ])\n",
        "    grid = {\"lasso__alpha\": np.logspace(-3, 1, 9)}\n",
        "    search = GridSearchCV(pipe, grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "    search.fit(X, y)\n",
        "    best = search.best_estimator_\n",
        "    rmse = -search.best_score_\n",
        "    mse = rmse**2\n",
        "    return best, rmse, mse\n",
        "\n",
        "def enet_tuned(pre):\n",
        "    pipe = Pipeline([\n",
        "        (\"pre_processing\", pre),\n",
        "        (\"elastic_net\", ElasticNet(max_iter=10000, random_state=0))\n",
        "    ])\n",
        "    grid = {\n",
        "        \"elastic_net__alpha\": np.logspace(-3, 1, 9),\n",
        "        \"elastic_net__l1_ratio\": np.linspace(0.1, 0.9, 9)\n",
        "    }\n",
        "    search = GridSearchCV(pipe, grid, scoring = \"neg_root_mean_squared_error\", cv = 5)\n",
        "    search.fit(X, y)\n",
        "    best = search.best_estimator_\n",
        "    rmse = -search.best_score_\n",
        "    mse = rmse**2\n",
        "    return best, rmse, mse"
      ],
      "id": "27a3781b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1 best numerical\n",
        "ct_1_num = ColumnTransformer([\n",
        "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), []),\n",
        "        (\"standardize\", StandardScaler(), best_num_1)\n",
        "    ])\n",
        "\n",
        "ols_1 = Pipeline([(\"pre_processing\", ct_1_num), (\"linear_regression\", LinearRegression())])\n",
        "ols_rmse_1, ols_mse_1 = evaluate_model(ols_1, X, y)\n",
        "\n",
        "ridge_best_1, ridge_rmse_1, ridge_mse_1 = ridge_tuned(ct_1_num)\n",
        "lasso_best_1, lasso_rmse_1, lasso_mse_1 = lasso_tuned(ct_1_num)\n",
        "enet_best_1, enet_rmse_1, enet_mse_1 = enet_tuned(ct_1_num)"
      ],
      "id": "1ccfa5d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# best 5 numericals\n",
        "ct_5_num = ColumnTransformer([\n",
        "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), []),\n",
        "        (\"standardize\", StandardScaler(), best_num_5)\n",
        "    ])\n",
        "\n",
        "ols_5 = Pipeline([(\"pre_processing\", ct_5_num), (\"linear_regression\", LinearRegression())])\n",
        "ols_rmse_5, ols_mse_5 = evaluate_model(ols_5, X, y)\n",
        "\n",
        "ridge_best_5, ridge_rmse_5, ridge_mse_5 = ridge_tuned(ct_5_num)\n",
        "lasso_best_5, lasso_rmse_5, lasso_mse_5 = lasso_tuned(ct_5_num)\n",
        "enet_best_5, enet_rmse_5, enet_mse_5 = enet_tuned(ct_5_num)"
      ],
      "id": "588e86f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# best 5 numericals + interaction + best categorical\n",
        "\n",
        "ct_inter = ColumnTransformer([\n",
        "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), best_cat_1),\n",
        "        (\"standardize_poly\", Pipeline([\n",
        "            (\"scale\", StandardScaler()),\n",
        "            (\"poly\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False))\n",
        "        ]), best_num_5)\n",
        "    ])\n",
        "\n",
        "ols_int = Pipeline([(\"pre_processing\", ct_inter), (\"linear_regression\", LinearRegression())])\n",
        "ols_rmse_inter, ols_mse_inter = evaluate_model(ols_int, X, y)\n",
        "\n",
        "ridge_best_inter, ridge_rmse_inter, ridge_mse_inter = ridge_tuned(ct_inter)\n",
        "lasso_best_inter, lasso_rmse_inter, lasso_mse_inter = lasso_tuned(ct_inter)\n",
        "enet_best_inter, enet_rmse_inter, enet_mse_inter = enet_tuned(ct_inter)"
      ],
      "id": "5d46747a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = pd.DataFrame({\n",
        "    \"Feature_Set\": [\"1 numeric\", \"5 numeric\", \"5 numeric + interactions + categorical\"],\n",
        "    \"OLS_MSE\": [ols_mse_1, ols_mse_5, ols_mse_inter],\n",
        "    \"Ridge_MSE\": [ridge_mse_1, ridge_mse_5, ridge_mse_inter],\n",
        "    \"LASSO_MSE\": [lasso_mse_1, lasso_mse_5, lasso_mse_inter],\n",
        "    \"ElasticNet_MSE\": [enet_mse_1, enet_mse_5, enet_mse_inter]\n",
        "})\n",
        "\n",
        "results"
      ],
      "id": "5b666a77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After re-tuning alpha and lambda for each feature set, the Elastic Net model using the five best numeric variables plus their interactions with Division achieved the lowest cross-validated MSE of approx. 100560. Also, Ridge regression on the same features performed almost identically.\n",
        "This indicates that incorporating a modest number of interaction terms together with balanced regularization yields the best generalization performance for predicting players' salaries.\n",
        "\n",
        "\n",
        "**PART 3: Discussion**\n",
        "\n",
        "A. Ridge:\n",
        "The Ridge coefficients were smaller in magnitude than the ordinary least-squares coefficients across all feature sets. This shrinkage makes sense because Ridge regression penalizes large coefficient values to reduce model variance and handle multicollinearity. While OLS sometimes produced very large positive and negative coefficients, especially when predictors were correlated, Ridge pulled them toward zero without eliminating them entirely. As a result, Ridge produced nearly identical but slightly lower MSE values than OLS, indicating better generalization through reduced variance.\n",
        "\n",
        "B: Lasso:\n",
        "The LASSO model in Part I and the three LASSO models in Part II did not use the same optimal lambda values. This makes sense since as the feature set changes, so does the overall scale and correlation structure of X, so the strength of the penalty required to balance bias and variance also changes. Also, the MSEs differed across feature sets. LASSO achieved much lower MSE when more relevant variables were added, and the smallest MSE when interactions were included.\n",
        "This also makes sense since adding predictive information gives LASSO more to work with, while its regularization still keeps uninformative coefficients at zero.\n",
        "\n",
        "C. Elastic Net:\n",
        "Across all feature sets, Elastic Net achieved the lowest (best) MSE, followed closely by Ridge.\n",
        "This outcome makes sense because Elastic Net combines the L2 penalty of Ridge (which stabilizes correlated predictors) with the L1 penalty of LASSO (which performs variable selection).\n",
        "In datasets like this baseball salary data—where many predictors are moderately correlated—Elastic Net gains the strengths of both methods: it can drop truly irrelevant features while still keeping correlated groups of variables together.\n",
        "Consequently, Elastic Net consistently “wins” by offering the best bias–variance trade-off and the most stable predictive performance.\n",
        "\n",
        "\n",
        "**PART 4: Final Model**"
      ],
      "id": "21c56abb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "enet_best_inter.fit(X, y)\n",
        "\n",
        "y_pred_final = enet_best_inter.predict(X)\n",
        "\n",
        "plot_df = pd.DataFrame({\"Actual\": y, \"Predicted\": y_pred_final})\n",
        "\n",
        "(\n",
        "    ggplot(plot_df, aes(x=\"Actual\", y=\"Predicted\"))\n",
        "    + geom_point()\n",
        "    + geom_abline()\n",
        "    + labs(\n",
        "        title=\"Final Elastic Net Model on Predicted vs Actual Salaries\",\n",
        "        x=\"Actual Salary ($ thousands)\",\n",
        "        y=\"Predicted Salary ($ thousands)\"\n",
        "    )\n",
        ")"
      ],
      "id": "3d1b576a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final model selected was an Elastic Net regression using the five most influential numeric predictors: Hits, AtBat, CRuns, CRBI, CWalks, as well as their interactions with the categorical variable Division.\n",
        "After tuning alpha and the lambda ratio in Part 2 and refitting this model on the full dataset, it achieved the lowest cross-validated MSE of approx 100560.\n",
        "The predicted-vs-actual salary plot shows a clear positive relationship and clustering along the line, indicating that the model captures the general salary structure well.\n",
        "Some underprediction for the very highest salaries reflects the regularization penalty’s effect of shrinking extreme coefficients toward zero.\n",
        "But overall, the Elastic Net model balances bias and variance effectively, producing accurate, stable, and interpretable predictions of baseball player salaries."
      ],
      "id": "419796b0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\ivpri\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}