{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LAB 7\n",
        "Priya Inampudi"
      ],
      "id": "3dc57e7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%echo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve, make_scorer, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "eb1eb14f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PART 1**"
      ],
      "id": "cc762f2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ha = pd.read_csv(\"https://www.dropbox.com/s/aohbr6yb9ifmc8w/heart_attack.csv?dl=1\")\n",
        "\n",
        "y = ha['output'].astype(int)\n",
        "X = ha.drop(columns=['output'])"
      ],
      "id": "a3145706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "id": "32bc74ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q1: KNN"
      ],
      "id": "b1d86cb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_pipe = Pipeline([\n",
        "    (\"standardize\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "knn_param_grid = {\n",
        "    \"knn__n_neighbors\": list(range(1, 41)),\n",
        "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
        "}\n",
        "\n",
        "knn_search = GridSearchCV(knn_pipe, knn_param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
        "\n",
        "knn_fitted = knn_search.fit(X_train, y_train)\n",
        "\n",
        "knn_best = knn_fitted.best_estimator_\n",
        "knn_cv_auc = knn_fitted.best_score_\n",
        "print(\"KNN best model:\", knn_fitted.best_params_)\n",
        "print(\"KNN CV AUC:\", knn_cv_auc)\n",
        "\n",
        "y_pred_knn = knn_best.predict(X_test)\n",
        "y_proba_knn = knn_best.predict_proba(X_test)[:,1]"
      ],
      "id": "feb8127e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KNN confusion matrix"
      ],
      "id": "0c4cce80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(confusion_matrix(y_test, y_pred_knn), index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"])"
      ],
      "id": "cc459f49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q2: logistic regression"
      ],
      "id": "fd86773d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logreg_pipe = Pipeline([\n",
        "    (\"standardize\", StandardScaler()),\n",
        "    (\"logreg\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
        "])\n",
        "\n",
        "logreg_param_grid = {\n",
        "    \"logreg__C\": np.logspace(-3, 3, 13),\n",
        "    \"logreg__penalty\": [\"l2\"] \n",
        "}\n",
        "\n",
        "logreg_search = GridSearchCV(logreg_pipe, logreg_param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
        "\n",
        "logreg_search.fit(X_train, y_train)\n",
        "\n",
        "logreg_best = logreg_search.best_estimator_\n",
        "logreg_cv_auc = logreg_search.best_score_\n",
        "print(\"LogReg best model:\", logreg_search.best_params_)\n",
        "print(\"LogReg CV AUC:\", logreg_cv_auc)\n",
        "\n",
        "y_pred_logreg = logreg_best.predict(X_test)\n",
        "\n",
        "y_proba_logreg = logreg_best.predict_proba(X_test)[:,1]"
      ],
      "id": "b4785e04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic regression confusion matric"
      ],
      "id": "2e87fe12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(confusion_matrix(y_test, y_pred_logreg), index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"])"
      ],
      "id": "0afec0ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q3: Decision Tree"
      ],
      "id": "04ada4fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "dt_param_grid = {\n",
        "    \"max_depth\": list(range(2, 16)),\n",
        "    \"min_samples_leaf\": [1, 2, 5, 10]\n",
        "}\n",
        "\n",
        "dt_search = GridSearchCV(dt, dt_param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
        "\n",
        "dt_search.fit(X_train, y_train)\n",
        "\n",
        "dt_best =dt_search.best_estimator_\n",
        "dt_cv_auc = dt_search.best_score_\n",
        "print(\"Decision tree best params:\", dt_search.best_params_)\n",
        "print(\"Decision tree CV AUC:\", dt_cv_auc)\n",
        "\n",
        "y_pred_dt = dt_best.predict(X_test)\n",
        "y_proba_dt = dt_best.predict_proba(X_test)[:,1]"
      ],
      "id": "7e02554e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "decision tree confusion matrix"
      ],
      "id": "a53f7e8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(confusion_matrix(y_test, y_pred_dt), index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"])"
      ],
      "id": "cce7b135",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q4: Interpretation\n",
        "\n",
        "\n",
        "Top logistic coefficients:"
      ],
      "id": "9a28ef0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logreg_model = logreg_best.named_steps[\"logreg\"]\n",
        "coef = pd.Series(logreg_model.coef_[0], index=X.columns).sort_values(key=np.abs, ascending=False)\n",
        "coef.head(8)"
      ],
      "id": "5450072e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Top decision tree important features:"
      ],
      "id": "1e1208aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "imp = pd.Series(dt_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "imp.head(8)"
      ],
      "id": "86c2efe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic model coefficient interpretation\n",
        "\n",
        "thalach: 1.07 --> Positive means higher maximum heart rate is associated with lower risk.\t\n",
        "\n",
        "sex: −0.98 --> Negative means being male or female substantially shifts risk\n",
        "\n",
        "cp: +0.78 --> Higher chest-pain type increases odds of being at risk.\n",
        "\n",
        "Smaller coefficients for trtbps, age, restecg, chol suggest minor effects after controlling for others.\n",
        "\n",
        "Most important predictors: Across all models, chest-pain type (cp) and maximum heart rate achieved (thalach) consistently emerged as the strongest predictors of heart-attack risk. Logistic regression additionally emphasized sex as an important variable.\n",
        "\n",
        "Q5: ROC curves plot"
      ],
      "id": "675f8d2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_proba_knn)\n",
        "fpr_logreg, tpr_logreg, thresholds_logreg = roc_curve(y_test, y_proba_logreg)\n",
        "fpr_dt, tpr_dt, threshold_dt = roc_curve(y_test, y_proba_dt)\n",
        "\n",
        "auc_knn  = roc_auc_score(y_test, y_proba_knn)\n",
        "auc_logreg  = roc_auc_score(y_test, y_proba_logreg)\n",
        "auc_dt = roc_auc_score(y_test, y_proba_dt)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "\n",
        "plt.plot(fpr_knn,  tpr_knn,  label=f\"KNN (AUC={auc_knn:.4f})\")\n",
        "\n",
        "plt.plot(fpr_logreg,  tpr_logreg,  label=f\"LogReg (AUC={auc_logreg:.4f})\")\n",
        "\n",
        "plt.plot(fpr_dt, tpr_dt, label=f\"Tree (AUC={auc_dt:.4f})\")\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--', linewidth=1)\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves for Heart Attack Risk across models\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "31b4ba97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PART 2**"
      ],
      "id": "e4848381"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tpr_scorer = make_scorer(recall_score)\n",
        "\n",
        "ppv_scorer = make_scorer(precision_score)\n",
        "\n",
        "tnr_scorer = make_scorer(recall_score, pos_label=0)\n",
        "\n",
        "models = {\n",
        "    \"KNN\": knn_best,\n",
        "    \"Logistic Regression\": logreg_best,\n",
        "    \"Decision Tree\": dt_best\n",
        "}\n",
        "\n",
        "metrics = []\n",
        "for name, model in models.items():\n",
        "    tpr = cross_val_score(model, X, y, cv=cv, scoring=tpr_scorer).mean()\n",
        "    ppv = cross_val_score(model, X, y, cv=cv, scoring=ppv_scorer).mean()\n",
        "    tnr = cross_val_score(model, X, y, cv=cv, scoring=tnr_scorer).mean()\n",
        "    metrics.append({\n",
        "        \"Model\": name,\n",
        "        \"True Positive Rate / Recall\": tpr,\n",
        "        \"Precision\": ppv,\n",
        "        \"True Negative Rate / Specificity\": tnr\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.round(4)"
      ],
      "id": "d906fd3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PART 3**\n",
        "\n",
        "Q1: I would aim to maximize recall. For this scenario I'd pick my logistic regression model. With recall being around 0.82, it detects most at-risk patients. Missing even one could be catastrophic, so we want to accept more false positives just for safety.\n",
        "\n",
        "Q2: I would aim to maximize precision and specificity, since I'd want to flag only the true high-risk cases. I would pick my decision tree model. A precision of 0.77 and specificity of 0.74 would mean fewer false alarms and more efficient bed use.\n",
        "\n",
        "Q3 (Studying root causes)\n",
        "\n",
        "Goal: I would use logistic regression so that for each cause I can interpret and understand the coefficients. They'll show how each factor changes heart-attack odds (e.g. cp, thalach, sex). \n",
        "\n",
        "Q4 (Training new doctors)\n",
        "\n",
        "Goal: I would use logistic regression to construct a stable, balanced reference model with good overall accuracy and AUC. For example with a AUC of 0.80 and balanced precision/recall, the model can be a consistent reference point to compare human diagnoses against and evaluating new doctors’ diagnostic skills.\n",
        "\n",
        "**PART 4**"
      ],
      "id": "7d580256"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ha_val = pd.read_csv(\"https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1\")\n",
        "ha_val.head()"
      ],
      "id": "11308dbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_val = ha_val.drop(columns=[\"output\"])\n",
        "y_val = ha_val[\"output\"].astype(int)"
      ],
      "id": "caeec5c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models = {\n",
        "    \"KNN\": knn_best,\n",
        "    \"Logistic Regression\": logreg_best,\n",
        "    \"Decision Tree\": dt_best\n",
        "}\n",
        "\n",
        "results_val = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_val)\n",
        "    y_proba = model.predict_proba(X_val)[:,1]\n",
        "\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    auc = roc_auc_score(y_val, y_proba)\n",
        "    prec = precision_score(y_val, y_pred)\n",
        "    rec = recall_score(y_val, y_pred)\n",
        "\n",
        "    print(\"\\n\", name)\n",
        "    print(\"Confusion Matrix:\\n\", pd.DataFrame(cm, index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"]))\n",
        "    print(f\"AUC = {auc:.4f}  Precision = {prec:.4f}  Recall = {rec:.4f}\")\n",
        "\n",
        "    results_val.append({\n",
        "        \"Model\": name,\n",
        "        \"AUC\": auc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec\n",
        "    })"
      ],
      "id": "13d7bfc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation Stats"
      ],
      "id": "b19bba77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "val_df = pd.DataFrame(results_val)\n",
        "val_df.round(4)"
      ],
      "id": "b85b777d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The validation set confirmed the stability of the three tuned models.\n",
        "\n",
        "- KNN achieved the highest validation AUC of 0.93, but performed very similarly to the logistic regression in terms of precision: 0.88 and recall: 0.79.\n",
        "\n",
        "- The decision tree model remained more conservative, yielding the highest precision of 0.93 but a lower recall of 0.74.\n",
        "\n",
        "These results mirror the cross-validated estimates from Part 2, suggesting that our cross-validation provided an accurate measure of real-world performance. Overall, the logistic regression model remains the preferred model, since its validation metrics align closely with CV results from parts 1 and 2. It gives us interpretability and balances sensitivity and specificity better than the other models.\n",
        "\n",
        "**PART 5**"
      ],
      "id": "3fc47e5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kappa_results = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_val)\n",
        "    kappa = cohen_kappa_score(y_val, y_pred)\n",
        "    kappa_results.append({\"Model\": name, \"Cohen Kappa\": kappa})\n",
        "\n",
        "kappa_df = pd.DataFrame(kappa_results)\n",
        "kappa_df.round(4)"
      ],
      "id": "4ea6590b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cohen’s Kappa was calculated to assess the agreement between predicted and actual classifications while adjusting for random chance.\n",
        "\n",
        "All three models demonstrated enough agreement. KNN and logistic regression both achieved a K value of 0.585, while the Decision Tree achieved 0.60.\n",
        "\n",
        "These results confirm our earlier resultss. Each model provides meaningful predictive ability, but the decision tree’s slightly higher Kappa suggests greater consistency in its predictions.\n",
        "However, this difference is small, and overall, the logistic regression model still remains preferred for its balance of performance, interpretability, and generalization.\n",
        "So the Kappa results do not change our previous conclusions but confirm that all three models are performing well above chance."
      ],
      "id": "384efb7e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\ivpri\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}